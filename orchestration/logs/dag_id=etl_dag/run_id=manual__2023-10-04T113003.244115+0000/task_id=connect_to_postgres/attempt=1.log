[2023-10-04T11:30:14.190+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_dag.connect_to_postgres manual__2023-10-04T11:30:03.244115+00:00 [queued]>
[2023-10-04T11:30:14.200+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_dag.connect_to_postgres manual__2023-10-04T11:30:03.244115+00:00 [queued]>
[2023-10-04T11:30:14.200+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2023-10-04T11:30:14.221+0000] {taskinstance.py:1380} INFO - Executing <Task(PythonOperator): connect_to_postgres> on 2023-10-04 11:30:03.244115+00:00
[2023-10-04T11:30:14.226+0000] {standard_task_runner.py:57} INFO - Started process 201 to run task
[2023-10-04T11:30:14.230+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_dag', 'connect_to_postgres', 'manual__2023-10-04T11:30:03.244115+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpd0l9xcud']
[2023-10-04T11:30:14.233+0000] {standard_task_runner.py:85} INFO - Job 11: Subtask connect_to_postgres
[2023-10-04T11:30:14.258+0000] {logging_mixin.py:151} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2023-10-04T11:30:14.310+0000] {task_command.py:415} INFO - Running <TaskInstance: etl_dag.connect_to_postgres manual__2023-10-04T11:30:03.244115+00:00 [running]> on host 9a46a127aae5
[2023-10-04T11:30:14.417+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_dag' AIRFLOW_CTX_TASK_ID='connect_to_postgres' AIRFLOW_CTX_EXECUTION_DATE='2023-10-04T11:30:03.244115+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-10-04T11:30:03.244115+00:00'
[2023-10-04T11:30:14.454+0000] {xcom.py:661} ERROR - Object of type connection is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config or make sure to decorate your object with attr.
[2023-10-04T11:30:14.457+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/json.py", line 91, in default
    return serialize(o)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/serialization/serde.py", line 178, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'psycopg2.extensions.connection'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/etl_dag.py", line 67, in _connect_to_postgres
    ti.xcom_push(key='db_connection', value=connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 2477, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/xcom.py", line 244, in set
    value = cls.serialize_value(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/xcom.py", line 659, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
  File "/usr/local/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/json.py", line 104, in encode
    return super().encode(o)
  File "/usr/local/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/json.py", line 93, in default
    return super().default(o)
  File "/usr/local/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type connection is not JSON serializable
[2023-10-04T11:30:14.483+0000] {taskinstance.py:1398} INFO - Marking task as FAILED. dag_id=etl_dag, task_id=connect_to_postgres, execution_date=20231004T113003, start_date=20231004T113014, end_date=20231004T113014
[2023-10-04T11:30:14.507+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 11 for task connect_to_postgres (Object of type connection is not JSON serializable; 201)
[2023-10-04T11:30:14.523+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-10-04T11:30:14.556+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
